{
 "cells": [
  {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ML4SCI/ML4SCIHackathon/blob/main/NMRSpinChallenge/Hackathon_NMR_Challenge_Nov2021.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Hackathon - NMR Challenge\n",
    "\n",
    "\n",
    "### Problem Statement\n",
    "\n",
    "- Nuclear Magnetic Resonance (NMR) is an experimental technique that allows for the control and measurement of nuclear spins in crystals and molecules.\n",
    "- A common \"recipe\" for NMR is called the spin echo: the spins start aligned, begin to disperse, and are then refocused. This creates a sharp peak, or \"echo\", in the net magnetization $M$ of the material at a later time. When the spins interact with each other, this refocused echo can become highly distorted.\n",
    "- Materials with strong electron-electron couplings have a variety of applications, from superconductivity to ferromagnetism. They also tend to enhance the nuclear spin-spin couplings, allowing NMR to act as a probe of these important systems.\n",
    "- Design and train a model that predicts the strength and shape of interactions between the nuclear spins from simulated time-dependent magnetization curves, $M(t)$.\n",
    "\n",
    "Before getting to any code, we first review the structure of this machine learning problem and introduce some of the details of the underlying physics we are trying to capture.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quick description of the ML problem\n",
    "\n",
    "### Goal:\n",
    "Predict three numbers from a large input vector of real numbers.\n",
    "\n",
    "### Example Solution:  Multilayer Neural Network (see code after this introduction)\n",
    "\n",
    "To get the complex-valued time-series $M(t)$ into a neural network, we can simply \"stack\" the real and complex parts together to make a real-valued input vector.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to NMR and spin echos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although the NMR \"spin echo\" technique may sound complicated, the following animation created by Gavin W Morley (by way of https://en.wikipedia.org/wiki/Spin_echo) makes it much clearer!\n",
    "\n",
    "\n",
    "![NMR Spin Echo Animation](HahnEcho_GWM.gif)\n",
    "\n",
    "\n",
    "The red arrows in this animation represents the values of nuclear spins in the material.\n",
    "They all begin in the same direction (up), and then an applied magnetic field rotates them into the x-y plane (indicated by the 90$^\\circ$ pulse).\n",
    "A constant external magnetic field in the z-direction did not affect the spins when they were pointing \"up\", but now that they lie in the x-y plane they begin to precess.\n",
    "\n",
    "\n",
    "Because each nuclear spin sits in a slightly different magnetic environment, each one has a slightly different response to the background z-direction magnetic field, causing some to precess in a clockwise direction and others in a counterclockwise direction.\n",
    "\n",
    "\n",
    "After a fixed amount of time, $t$ in the above animation, a second magnetic pulse is applied and rotates each spin 180$^\\circ$ in the x-y plane.\n",
    "After this, the spins continue to move as they did before, but because of the 180$^\\circ$ pulse they are now effectively precessing  \"backwards\" compared to the original motion!\n",
    "So after an additional time $t$ passes, the variations in precession time is canceled out, causing a refocusing of the spins.\n",
    "\n",
    "This shows up as a measurable \"echo\" in the average spin magnetization of the material, and can be measured in experiments.\n",
    "This is an important technique because the average spin magnetization is hard to measure during an applied \"pulse\", but there is no external pulse during the \"echo\", allowing for accurate measurement of the peak value and decay shape.\n",
    "\n",
    "In the following, we will use simulated data that studies how the magnetization depends on the interaction between spins, $\\alpha$, and the effective flip angle for the first pulse, $\\theta$.\n",
    "In spin echo experiments, one tries to apply a $\\theta = $ 90$^\\circ$ pulse as in the above example, but in practice it is a hard thing to do!\n",
    "Therefore, we will consider cases where the spins are not always exactly inplane after the first pulse ($\\theta \\neq $ 90$^\\circ$).\n",
    "For example, the following diagram shows a pulse angle of $\\theta \\approx $ 40$^\\circ$.\n",
    "\n",
    "![spin_interaction_diagram.png](spin_interaction_diagram.png)\n",
    "\n",
    "\n",
    "Here are examples of the time-dependent in-plane magnetization $M(t) = M_x(t) + i M_y(t)$ for a spin-echo materials ($M_x$ in red, $M_y$ in blue):\n",
    "\n",
    "![standard_spinecho.png](spin_echo_examples.png)\n",
    "\n",
    "The first example is a fairly standard spin echo: the average magnetization decays quite fast after the first $\\theta \\approx 90^\\circ$ pulse, and re-appears an equal time after the $2\\theta \\approx 180^\\circ$ pulse as an \"echo\"!\n",
    "\n",
    "The second more complicated example have been caused by spin-spin interactions between the precessing nuclear spins and a flip-angle $\\theta$ not near 90$^\\circ$. Normally, each spin precesses in a uniform way irrespective of the rest of the nuclei in the material. In this coupled case, however, the nuclear magnetization that occurs near the \"echo\" influences the spins' motions, modifying the shape of the observed echo.\n",
    "\n",
    "The final case shows all 3,000 $M(t)$ curves from the simualtion dataset we will be using. Since most experiments do not have access to low-noise data near the $\\theta$ and $2\\theta$ pulse times, we will only be using the region given within the green lines, $t \\in [130,270]$ $\\mu$s."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Electronic and nuclear spins\n",
    "\n",
    "Most materials can be classified by their electronic properties into three categories: metal, insulator, and semiconductor.\n",
    "These terms are based on a semi-classical description of the electrons in a crystal.\n",
    "The electrons are treated as a collection of classical particles, with energies that depend on their momentum in a way determined by the atomic structure of the crystal.\n",
    "\n",
    "However, there are other electronic phases of matter that are truly \"quantum\" and cannot be described accurately with a classical analogy.\n",
    "In these scenarios, complicated structures in the electron states can give rise to large electronic spin density or strong electron-electron coupling.\n",
    "Because of these strong couplings between electrons, they are often hard to probe experimentally.\n",
    "\n",
    "Luckily, electrons can interact with the nuclear spins of a material (by way of the hyperfine-interaction).\n",
    "If the electron-nuclear coupling becomes strong enough (mediated perhaps by a strongly correlated electronic phase), then the nuclear spins will couple throughout the material.\n",
    "This can be viewed as a two-step process, where a nuclear spin couples to an electron and changes its motion, and then that electron later \"scatters\" off another nuclear spin elsewhere in the material.\n",
    "\n",
    "We here make the simplifying assumption that this scattering effect is determined by only three variables:\n",
    "\n",
    "$\\alpha_x$: The effective scattering strength for nuclear spins when the spin-axis is perpindicular to the $z$-direction (perpindicular to the constant magnetic field). Note that we call this $\\alpha_x$ but it defines the coupling in the entire $xy$ plane.\n",
    "\n",
    "$\\alpha_z$: The effective scattering strength for nuclear spins when the spin-axis is along the $z$-direction (parallel to the constant magnetic field)\n",
    "\n",
    "$\\theta$: The \"flip angle\" of the nuclear spins caused by the strength of the applied pulse at $t=0$ and $t=\\tau$. In an ideal spin-echo experiment, $\\theta =$ 90$^\\circ$, but in practice the exact flip-angle is never known.\n",
    "\n",
    "\n",
    "Here we provide simulated spin-echo magnetizations $M(t)$ from random samples of these three parameters to see if the spin-echo experiment can provide enough information to accurately \"reverse engineer\" them from a single $M(t)$ curve.\n",
    "\n",
    "\n",
    "In the simulated data-sets, the parameters are randomly distributed over the following ranges:\n",
    "- $\\alpha_x$: coupling strength $\\in$ [0, 100] kHz\n",
    "- $\\alpha_z$: coupling strength $\\in$ [0, 300] kHz\n",
    "- $\\theta$: flip angle $\\in$ [10$^\\circ$,90$^\\circ$]\n",
    "\n",
    "Our goal is to develop a model that accurately determines the above three variables from a single $M(t)$ curve.\n",
    "\n",
    "For more detail on the coupled spil model and details of the simulations, see the recent preprint: https://arxiv.org/abs/2110.06811"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and view the simulated data\n",
    "\n",
    "Three data-files will be used for the training of the model. Each has 3,000 lines, representing 3,000 simulated $M(t)$ curves for different choices of the three material parameters:\n",
    "\n",
    "- echos_r.txt  : Real part of the time-dependent magnetization, $\\textrm{Re}(M(t))$.\n",
    "- echos_i.txt  : Imaginary part of the time-dependent magnetization, $\\textrm{Im}(M(t))$.\n",
    "- mat_info.txt : The three material parameters ($\\alpha_x$,$\\alpha_z$,$\\theta$) introduced above.\n",
    "\n",
    "We also download two smaller files, which give the 1000 $M(t)$ curves to be used in submitting your solution to the challenge:\n",
    "\n",
    "- submit_echos_r.txt\n",
    "- submit_echos_i.txt"
   ]
  },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mm3qvRtKeMk-"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import requests\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "\n",
        "print(\"Downloading files off google drive...\")\n",
        "\n",
        "f_prefix = \"gauss\"\n",
        "\n",
        "# data for model creation\n",
        "gauss_train_y = f_prefix+\"_mat_info_model.txt\"\n",
        "gauss_train_X_real = f_prefix+\"_echos_model_r.txt\" # real part of echos\n",
        "gauss_train_X_imaginary = f_prefix+\"_echos_model_i.txt\" # imaginary part of echos\n",
        "\n",
        "asdf = \"https://drive.google.com/uc?export=download&id=\"\n",
        "\n",
        "r = requests.get(\"https://drive.google.com/uc?export=download&id=1J8CcJVQRpzSwue1vuHV9uB0bngdDrKCY\",allow_redirects=True)\n",
        "open(gauss_train_y, \"wb\").write(r.content)\n",
        "r = requests.get(\"https://drive.google.com/uc?export=download&id=1lBWcwF--1rrB8KCyCd0-5ZnPIjRrWkHg\",allow_redirects=True)\n",
        "open(gauss_train_X_real, \"wb\").write(r.content)\n",
        "r = requests.get(\"https://drive.google.com/uc?export=download&id=1O7KKL-SW3vHePoRNk8YfLzX82wf2Z5ul\",allow_redirects=True)\n",
        "open(gauss_train_X_imaginary, \"wb\").write(r.content)\n",
        "\n",
        "# data for submission of final model\n",
        "gauss_test_X_real = f_prefix+\"_echos_eval_r.txt\" # real part of echos\n",
        "gauss_test_X_imaginary = f_prefix+\"_echos_eval_i.txt\" # imaginary part of echos\n",
        "\n",
        "r = requests.get(\"https://drive.google.com/uc?export=download&id=1prIrtO7XJs3PBe1MZiWUxK3VUkrChVvz\",allow_redirects=True)\n",
        "open(gauss_test_X_real, \"wb\").write(r.content)\n",
        "r = requests.get(\"https://drive.google.com/uc?export=download&id=1vbKcuxe6z8cRGQdTqj_Q2u5Oow0D9hbU\",allow_redirects=True)\n",
        "open(gauss_test_X_imaginary, \"wb\").write(r.content)\n",
        "\n",
        "# now repeat, but for RKKY type function\n",
        "\n",
        "f_prefix = \"RKKY\"\n",
        "\n",
        "# data for model training\n",
        "rrky_train_y = f_prefix+\"_mat_info_model.txt\"\n",
        "rrky_train_X_real = f_prefix+\"_echos_model_r.txt\" # real part of echos\n",
        "rrky_train_X_imaginary = f_prefix+\"_echos_model_i.txt\" # imaginary part of echos\n",
        "r = requests.get(\"https://docs.google.com/uc?export=download&id=1lS9AJ3sUFI4cfM5jQj618x4shoaJMXVo\",allow_redirects=True)\n",
        "open(rrky_train_y, \"wb\").write(r.content)\n",
        "r = requests.get(\"https://docs.google.com/uc?export=download&id=1J21bKy8FTjoaGzHVdLXlWAao2UiWO7ml\",allow_redirects=True)\n",
        "open(rrky_train_X_real, \"wb\").write(r.content)\n",
        "r = requests.get(\"https://docs.google.com/uc?export=download&id=1nf3Y_FcJJEWXJbjwREAkgcnVz2tDA__I\",allow_redirects=True)\n",
        "open(rrky_train_X_imaginary, \"wb\").write(r.content)\n",
        "\n",
        "# data for submission of final model\n",
        "rrky_test_X_real = f_prefix+\"_echos_eval_r.txt\" # real part of echos\n",
        "rrky_test_X_imaginary = f_prefix+\"_echos_eval_i.txt\" # imaginary part of echos\n",
        "\n",
        "r = requests.get(\"https://docs.google.com/uc?export=download&id=1Q46o_RnYZFWEjMVVF5m1VBI9HCltspyY\",allow_redirects=True)\n",
        "open(rrky_test_X_real, \"wb\").write(r.content)\n",
        "r = requests.get(\"https://docs.google.com/uc?export=download&id=1-z2ADFrBlEhXN5Z_LHiRLA4Nds_9uvQq\",allow_redirects=True)\n",
        "open(rrky_test_X_imaginary, \"wb\").write(r.content)\n",
        "\n",
        "\n",
        "print(\"Done with file downloads\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "gauss_train_X_real = np.loadtxt(gauss_train_X_real, comments=\"#\", delimiter=None, unpack=False)\n",
        "gauss_train_X_imaginary = np.loadtxt(gauss_train_X_imaginary, comments=\"#\", delimiter=None, unpack=False)\n",
        "gauss_train_y = np.loadtxt(gauss_train_y, comments=\"#\", delimiter=None, unpack=False)\n",
        "\n",
        "gauss_test_X_real = np.loadtxt(gauss_test_X_real, comments=\"#\", delimiter=None, unpack=False)\n",
        "gauss_test_X_imaginary = np.loadtxt(gauss_test_X_imaginary, comments=\"#\", delimiter=None, unpack=False)\n",
        "\n",
        "rkky_train_X_real = np.loadtxt(rrky_train_X_real, comments=\"#\", delimiter=None, unpack=False)\n",
        "rkky_train_X_imaginary = np.loadtxt(rrky_train_X_imaginary, comments=\"#\", delimiter=None, unpack=False)\n",
        "rkky_train_y = np.loadtxt(rrky_train_y, comments=\"#\", delimiter=None, unpack=False)\n",
        "\n",
        "rkky_test_X_real = np.loadtxt(rrky_test_X_real, comments=\"#\", delimiter=None, unpack=False)\n",
        "rkky_test_X_imaginary = np.loadtxt(rrky_test_X_imaginary, comments=\"#\", delimiter=None, unpack=False)"
      ],
      "metadata": {
        "id": "xogFKL42TUwJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## IMPORTANT NOTE IF YOU WANT TO SAVE A LOT OF TIME WHILE TESTING THINGS\n",
        "\n",
        "The full time series contain a lot of information that might not actually be <br>\n",
        "useful. Here the key region of interest is around 200-400 so you could cut <br>\n",
        "down the length of the time series to that window. It may slightly worsen <br>\n",
        "final model performance but can save a ton of time.<br>\n",
        "e.g. `gauss_train_X_imaginary = gauss_train_X_imaginary[:,200:400]`"
      ],
      "metadata": {
        "id": "8HLRFN3Zf3NR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot((gauss_train_X_imaginary[1:500,:]).T,color='orange', alpha=0.05)\n",
        "plt.plot((gauss_train_X_real[1:500,:]).T,color='blue', alpha=0.05)\n",
        "plt.xlabel(\"$t$\")\n",
        "plt.ylabel(\"$M$\")\n",
        "plt.legend()\n",
        "plt.title(\"Training set with a full time axis\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "CYo0ZFV0dD49"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# rescale data\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "gauss_X_sc_real = StandardScaler()\n",
        "gauss_train_X_real = gauss_X_sc_real.fit_transform(gauss_train_X_real)\n",
        "gauss_test_X_real = gauss_X_sc_real.transform(gauss_test_X_real)\n",
        "gauss_X_sc_imaginary = StandardScaler()\n",
        "gauss_train_X_imaginary = gauss_X_sc_imaginary.fit_transform(gauss_train_X_imaginary)\n",
        "gauss_test_X_imaginary = gauss_X_sc_imaginary.transform(gauss_test_X_imaginary)\n",
        "gauss_y_sc = StandardScaler()\n",
        "gauss_train_y = gauss_y_sc.fit_transform(gauss_train_y)\n",
        "\n",
        "rkky_X_sc_real = StandardScaler()\n",
        "rkky_train_X_real = rkky_X_sc_real.fit_transform(rkky_train_X_real)\n",
        "rkky_test_X_real = rkky_X_sc_real.transform(rkky_test_X_real)\n",
        "rkky_X_sc_imaginary = StandardScaler()\n",
        "rkky_train_X_imaginary = rkky_X_sc_imaginary.fit_transform(rkky_train_X_imaginary)\n",
        "rkky_test_X_imaginary = rkky_X_sc_imaginary.transform(rkky_test_X_imaginary)\n",
        "rkky_y_sc = StandardScaler()\n",
        "rkky_train_y = rkky_y_sc.fit_transform(rkky_train_y)\n",
        "\n",
        "# partition data into a training and testing set using a random partition\n",
        "from sklearn.model_selection import train_test_split\n",
        "gauss_train_X_real, gauss_valid_X_real = train_test_split(gauss_train_X_real, test_size=0.1, random_state=42)\n",
        "gauss_train_X_imaginary, gauss_valid_X_imaginary = train_test_split(gauss_train_X_imaginary, test_size=0.1, random_state=42)\n",
        "gauss_train_y, gauss_valid_y = train_test_split(gauss_train_y, test_size=0.1, random_state=42)\n",
        "rkky_train_X_real, rkky_valid_X_real = train_test_split(rkky_train_X_real, test_size=0.1, random_state=42)\n",
        "rkky_train_X_imaginary, rkky_valid_X_imaginary = train_test_split(rkky_train_X_imaginary, test_size=0.1, random_state=42)\n",
        "rkky_train_y, rkky_valid_y = train_test_split(rkky_train_y, test_size=0.1, random_state=42)"
      ],
      "metadata": {
        "id": "92Qjtn9oSgeY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TwoChannelTimeSeries(Dataset):\n",
        "    def __init__(self, X, y=None):\n",
        "        self.X = np.stack(X, axis=2)\n",
        "        self.X = torch.from_numpy(self.X.copy()).float()\n",
        "        if y is not None:\n",
        "            self.y = torch.from_numpy(y.copy()).float()\n",
        "        else:\n",
        "            self.y = None\n",
        "    def __len__(self):\n",
        "        return len(self.X)\n",
        "    def __getitem__(self, idx):\n",
        "        if self.y is None:\n",
        "            return self.X[idx]\n",
        "        return self.X[idx], self.y[idx]\n",
        "\n",
        "batch_size = 128\n",
        "\n",
        "train_data = TwoChannelTimeSeries([gauss_train_X_real, gauss_train_X_imaginary], gauss_train_y)\n",
        "valid_data = TwoChannelTimeSeries([gauss_valid_X_real, gauss_valid_X_imaginary], gauss_valid_y)\n",
        "test_data = TwoChannelTimeSeries([gauss_test_X_real, gauss_test_X_imaginary], None)\n",
        "\n",
        "gauss_train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
        "gauss_valid_loader = DataLoader(valid_data, batch_size=batch_size, shuffle=False)\n",
        "gauss_test_loader = DataLoader(test_data, batch_size=32, shuffle=False)\n",
        "\n",
        "train_data = TwoChannelTimeSeries([rkky_train_X_real, rkky_train_X_imaginary], rkky_train_y)\n",
        "valid_data = TwoChannelTimeSeries([rkky_valid_X_real, rkky_valid_X_imaginary], rkky_valid_y)\n",
        "test_data = TwoChannelTimeSeries([rkky_test_X_real, rkky_test_X_imaginary], None)\n",
        "\n",
        "rkky_train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
        "rkky_valid_loader = DataLoader(valid_data, batch_size=batch_size, shuffle=False)\n",
        "rkky_test_loader = DataLoader(test_data, batch_size=32, shuffle=False)"
      ],
      "metadata": {
        "id": "PudOUbQWQo5w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_and_validate(train_loader, val_loader, model, optimizer, criterion, num_epochs, metric=None, scheduler=None, device='cpu'):\n",
        "    history = {\n",
        "        'epoch': [],\n",
        "        'train_loss': [],\n",
        "        'train_metric': [],\n",
        "        'val_loss': [],\n",
        "        'val_metric': [],\n",
        "        'learning_rate': []\n",
        "    }  # Initialize a dictionary to store epoch-wise results\n",
        "\n",
        "    model.to(device)  # Move the model to the specified device\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()  # Set the model to training mode\n",
        "        epoch_loss = 0.0  # Initialize the epoch loss and metric values\n",
        "        epoch_metric = 0.0\n",
        "\n",
        "        # Training loop\n",
        "        for X, y in train_loader:\n",
        "            X = X.to(device)\n",
        "            y = y.to(device)\n",
        "            y = y\n",
        "            optimizer.zero_grad()  # Clear existing gradients\n",
        "            outputs = model(X)  # Make predictions\n",
        "            loss = criterion(outputs, y)  # Compute the loss\n",
        "            loss.backward()  # Compute gradients\n",
        "            optimizer.step()  # Update model parameters\n",
        "\n",
        "            epoch_loss += loss.item()\n",
        "\n",
        "            # THESE LINES HAVE BEEN UPDATED TO ACCOUNT FOR DEFAULT ARGUMENTS\n",
        "            if metric is not None:\n",
        "                epoch_metric += metric(outputs, y)\n",
        "            else:\n",
        "                epoch_metric += 0.0\n",
        "\n",
        "        # Average training loss and metric\n",
        "        epoch_loss /= len(train_loader)\n",
        "        epoch_metric /= len(train_loader)\n",
        "\n",
        "        # Validation loop\n",
        "        model.eval()  # Set the model to evaluation mode\n",
        "        with torch.no_grad():  # Disable gradient calculation\n",
        "            val_loss = 0.0\n",
        "            val_metric = 0.0\n",
        "            for X_val, y_val in val_loader:\n",
        "                X_val = X_val.to(device)\n",
        "                y_val = y_val.to(device)\n",
        "                y_val = y_val\n",
        "                outputs_val = model(X_val)  # Make predictions\n",
        "                val_loss += criterion(outputs_val, y_val).item()  # Compute loss\n",
        "                if metric is not None:\n",
        "                    val_metric += metric(outputs_val, y_val)\n",
        "                else:\n",
        "                    val_metric += 0.0\n",
        "\n",
        "            val_loss /= len(val_loader)\n",
        "            val_metric /= len(val_loader)\n",
        "\n",
        "        # Append epoch results to history\n",
        "        history['epoch'].append(epoch)\n",
        "        history['train_loss'].append(epoch_loss)\n",
        "        history['train_metric'].append(epoch_metric)\n",
        "        history['val_loss'].append(val_loss)\n",
        "        history['val_metric'].append(val_metric)\n",
        "        history['learning_rate'].append(optimizer.param_groups[0]['lr'])\n",
        "\n",
        "        print(f'Epoch [{epoch+1}/{num_epochs}], Train Loss: {epoch_loss:.4f}, '\n",
        "              f'Train Metric: {epoch_metric:.4f}, Val Loss: {val_loss:.4f}, '\n",
        "              f'Val Metric: {val_metric:.4f}')\n",
        "\n",
        "        if scheduler is not None:\n",
        "            scheduler.step()\n",
        "\n",
        "    return history, model\n",
        "\n",
        "\n",
        "def test_model(model, data_loader, criterion, metric=None, device='cpu'):\n",
        "    model.to(device)  # Move the model to the specified device\n",
        "\n",
        "    model.eval()  # Set the model to evaluation mode\n",
        "\n",
        "    total_loss = 0.0  # Initialize the total loss and metric values\n",
        "    total_metric = 0.0\n",
        "\n",
        "\n",
        "    with torch.no_grad():  # Disable gradient tracking\n",
        "        for batch in data_loader:\n",
        "            X, y = batch\n",
        "            X = X.to(device)\n",
        "            y = y.to(device)\n",
        "            # Pass the data to the model and make predictions\n",
        "            outputs = model(X)\n",
        "\n",
        "            # Compute the loss\n",
        "            loss = criterion(outputs, y)\n",
        "\n",
        "            # Add the loss and metric for the batch to the total values\n",
        "            total_loss += loss.item()\n",
        "\n",
        "            if metric is not None:\n",
        "                total_metric += metric(outputs, y)\n",
        "            else:\n",
        "                total_metric += 0.0\n",
        "\n",
        "    # Average loss and metric for the entire dataset\n",
        "    avg_loss = total_loss / len(data_loader)\n",
        "    avg_metric = total_metric / len(data_loader)\n",
        "\n",
        "    print(f'Test Loss: {avg_loss:.4f}, Test Metric: {avg_metric:.4f}')\n",
        "\n",
        "    return avg_loss, avg_metric"
      ],
      "metadata": {
        "id": "Bze1XMHiXeKI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "id": "bb9p8ytmXhuW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RliStr5CeMlC"
      },
      "source": [
        "## Example solution: a simple neural net (NN)\n",
        "- Our input nodes are the vector $[\\textrm{Re}(M(t)), \\textrm{Im}(M(t))]$, which is a few hundred elements.\n",
        "- Our output nodes are the three material parameters.\n",
        "- We will use a standard NN predict the material properties from $M(t)$.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nFdUzF8yeMlC"
      },
      "outputs": [],
      "source": [
        "# first we build the model\n",
        "\n",
        "N = np.shape(next(iter(gauss_train_loader))[0])[1] # number of input values from M(t) curve\n",
        "\n",
        "# define the net\n",
        "simplenn = nn.Sequential()\n",
        "# Let's try N -> 100 -> 40 -> 3, e.g. 2 hidden layers\n",
        "simplenn.append(nn.Flatten())\n",
        "simplenn.append(nn.Linear(N*2, 100))\n",
        "simplenn.append(nn.ReLU())\n",
        "simplenn.append(nn.Linear(100, 40))\n",
        "simplenn.append(nn.ReLU())\n",
        "simplenn.append(nn.Linear(40, 3))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "csDDTh1TeMlC"
      },
      "outputs": [],
      "source": [
        "optimizer = torch.optim.Adam(simplenn.parameters(), lr=0.0001)\n",
        "criterion = nn.MSELoss()\n",
        "# now train it\n",
        "history, simplenn = train_and_validate(gauss_train_loader, gauss_valid_loader, simplenn, optimizer, criterion, num_epochs=200, device=device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oNmHuEm0eMlD"
      },
      "outputs": [],
      "source": [
        "# check results on test set\n",
        "results = test_model(simplenn, gauss_valid_loader, criterion, device=device)\n",
        "print(\"test loss:\", results)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "simplenn.to('cpu')\n",
        "simplenn.eval()\n",
        "predictions = [simplenn(x[0]).detach().numpy() for x in gauss_valid_loader]\n",
        "predictions = np.concatenate(predictions, axis=0)\n",
        "predictions = gauss_y_sc.inverse_transform(predictions)\n",
        "true = gauss_y_sc.inverse_transform(gauss_valid_loader.dataset.y.numpy())"
      ],
      "metadata": {
        "id": "Qe3FcEcWZaM7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model performance visualization\n",
        "\n",
        "The quantities you are trying to predict and the associated predictions are <br>\n",
        "shown below. They are `alpha`, `xi`, and `d`. Below true and predicted values <br>\n",
        "are compared using a correlation plot."
      ],
      "metadata": {
        "id": "rdwjtKVEg8Hr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.scatter(predictions[:,0],true[:,0]);\n",
        "plt.plot([-100,100],[-100, 100],\"--k\")\n",
        "plt.xlabel(\"True alpha\");\n",
        "plt.ylabel(\"Predicted alpha\");\n",
        "plt.axis([-2, 24, -2, 24])\n",
        "plt.title(\"Correlation strength\")\n",
        "\n",
        "plt.figure()\n",
        "plt.scatter(predictions[:,1],true[:,1]);\n",
        "plt.plot([-100, 100],[-100, 100],\"--k\")\n",
        "plt.xlabel(\"True xi\");\n",
        "plt.ylabel(\"Predicted xi\");\n",
        "plt.axis([0, 70, 0, 70])\n",
        "plt.title(\"Correlation length\")\n",
        "\n",
        "plt.figure()\n",
        "plt.scatter(predictions[:,2],true[:,2]);\n",
        "plt.plot([-100, 100],[-100, 100],\"--k\")\n",
        "plt.xlabel(\"True d\");\n",
        "plt.ylabel(\"Predicted d\");\n",
        "plt.axis([5, 12, 5, 12])\n",
        "plt.title(\"Dissipation strength\")"
      ],
      "metadata": {
        "id": "lvXkonAxZSxx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mkipX19ReMlD"
      },
      "source": [
        "## Submission format\n",
        "\n",
        "__We ask you to submit two models: one for guass and one for rkky__\n",
        "\n",
        "We ask you to make your predictions on the test sets. You don't have the true <br>\n",
        "labels for the test sets meaning you are limited only to what you know about <br>\n",
        "train sets (and validation subsets) to build the best models possible.\n",
        "\n",
        "Use your models to predict the three spin-interaction variables from the  <br>\n",
        "echos, and submit your results for **each model** in a tab delimited .txt  <br>\n",
        "file of dimensions 6000 x 3 matching the \"\\<model_type\\>\\_mat_info_model.txt\" format.\n",
        "\n",
        "That is, the columns should be:\n",
        "\n",
        "| $\\alpha$ | $\\xi$ | $d$ |\n",
        "      \n",
        "and there should be 6000 rows.\n",
        "\n",
        "Name this file \"\\<model_type\\>_mat_info_eval.txt\"\n",
        "\n",
        "The quality of the model will be judged by the minimization of normalized <br> mean-square error:\n",
        "\n",
        "\n",
        "$\\textrm{Err} = \\sum_{v=1}^{3} \\sum_{i=i}^{6000} \\left( \\tilde{v}^i_\\textrm{model} - \\tilde{v}^i_\\textrm{true} \\right)^2 $\n",
        "\n",
        "where $v^i$ is one of the three spin-interaction variables for echo number $i$, <br>\n",
        "and the tilde represents normalization of each variable (using the <br> StandardScaler() object used above).\n",
        "\n",
        "\n",
        "Your submission should include: <br>\n",
        "- Your ipython notebook (`.ipynb`),<br>\n",
        "- A PDF copy of your notebook together with a description of what you have done,<br>\n",
        "- Your model's evaluation of the Gaussian data (\"gauss_mat_info_eval.txt\"), <br>\n",
        "- Your model's evaluation of the RKKY data (\"RKKY_mat_info_eval.txt\").\n",
        "\n",
        "**NOTE: If your final model prediction files aren't named like** <br>\n",
        "**\"gauss_mat_info_eval.txt\" and \"RKKY_mat_info_eval.txt\" your results cannot be**<br>\n",
        "**counted for hackathon rankings.**\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.0"
    },
    "latex_envs": {
      "LaTeX_envs_menu_present": true,
      "autoclose": false,
      "autocomplete": true,
      "bibliofile": "biblio.bib",
      "cite_by": "apalike",
      "current_citInitial": 1,
      "eqLabelWithNumbers": true,
      "eqNumInitial": 1,
      "hotkeys": {
        "equation": "Ctrl-E",
        "itemize": "Ctrl-I"
      },
      "labels_anchors": false,
      "latex_user_defs": false,
      "report_style_numbering": false,
      "user_envs_cfg": false
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
