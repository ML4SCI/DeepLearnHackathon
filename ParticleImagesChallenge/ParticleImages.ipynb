{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ML4SCI/DeepLearnHackathon/blob/main/ParticleImagesChallenge/ParticleImages.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P3DtjEbf8M4I"
      },
      "source": [
        "**Introduction**\n",
        "\n",
        "Machine Learning algorithms have become an increasingly important tool for analyzing the data from the Large Hadron Collider (LHC). Identification of particles in LHC collisions is an important task of LHC detector reconstruction algorithms.\n",
        "\n",
        "Here we present a challenge where one of the detectors (the Electromagnetic Calorimeter or ECAL) is used as a camera to analyze detector images from two types of particles: electrons and photons that deposit their energy in this detector.\n",
        "\n",
        "**Dataset**\n",
        "\n",
        "Each pixel in the image corresponds to a detector cell, while the intensity of the pixel corresponds to how much energy is measured in that cell. Timing of the energy deposits are also available, though this may or may not be relevant. The dataset contains 32x32 Images of the energy hits and their timing (channel 1: hit energy and channel 2: its timing) in each calorimeter cell (one cell = one pixel) for the two classes of particles: Electrons and Photons. The dataset contains around four hundred thousand images for electrons and photons. Please note that your final model will be evaluated on an unseen test dataset.\n",
        "\n",
        "**Algorithm**\n",
        "\n",
        "Please use a Machine Learning model of your choice to achieve the highest possible classification performance on the provided dataset. Please provide a Jupyter Notebook that shows your solution.\n",
        "\n",
        "Evaluation Metrics\n",
        "ROC curve (Receiver Operating Characteristic curve) and AUC score (Area Under the ROC Curve)\n",
        "Training and Validation Accuracy\n",
        "The model performance will be tested on a hidden test dataset based on the above metrics.\n",
        "\n",
        "**Deliverables**\n",
        "\n",
        "Fill out the pre- and post-hackathon surveys.\n",
        "Pdf and .ipynb file showing your solution along with the final model accuracy (Training and Validation), ROC curve and AUC score. More details regarding the format of the notebook can be found in the sample Google Colab notebook provided for this challenge.\n",
        "The final trained model including the model architecture and the trained weights ( For example: HDF5 file, .pb file, .pt file, etc. ). You are free to choose Machine Learning Framework of your choice.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g0b6SRpWl2Xh"
      },
      "source": [
        "## Create the appropriate project folder"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "ycLTR9DCyIyP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/drive/MyDrive/"
      ],
      "metadata": {
        "id": "er-ovBUryJxj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OYRULwKaw_A6"
      },
      "outputs": [],
      "source": [
        "mkdir Particle_Images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "at-4-Xub8DYW"
      },
      "outputs": [],
      "source": [
        "cd Particle_Images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BQGKM10j2CiQ"
      },
      "outputs": [],
      "source": [
        "mkdir data/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PEtRyfNv9XVn"
      },
      "source": [
        "# Download the Dataset\n",
        "This will download 83MB for SingleElectron and 76MB for SinglePhoton."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rcK1wY4Qt_7Y"
      },
      "outputs": [],
      "source": [
        "#!/bin/bash\n",
        "!wget https://cernbox.cern.ch/index.php/s/sHjzCNFTFxutYCj/download -O data/SingleElectronPt50_IMGCROPS_n249k_RHv1.hdf5\n",
        "!wget https://cernbox.cern.ch/index.php/s/69nGEZjOy3xGxBq/download -O data/SinglePhotonPt50_IMGCROPS_n249k_RHv1.hdf5"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BepRE7pn8Du7"
      },
      "source": [
        "# Import modules"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZnLzC5paz0hb"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "np.random.seed(42)\n",
        "import torch\n",
        "torch.manual_seed(42)\n",
        "torch.cuda.manual_seed_all\n",
        "import h5py\n",
        "from torch import nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "from sklearn.metrics import roc_curve, auc\n",
        "\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HBsy7x5O8cWH"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Az_MoJwZ8K6l"
      },
      "source": [
        "# Pytorch Model Parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZzF8AHl_4yUA"
      },
      "outputs": [],
      "source": [
        "lr_init     = 1e-4    # Initial learning rate\n",
        "batch_size  = 64      # Training batch size\n",
        "num_epochs = 10       # Number of training epochs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_jX_l-WmplJx"
      },
      "source": [
        "It is recommended to use GPU for training and inference if possible."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xwc56kXJ8TLo"
      },
      "source": [
        "# Load Image Data\n",
        "- Two classes of particles: electrons and photons\n",
        "- 32x32 matrices (two channels - hit energy and time) for the two classes of particles electrons and photons impinging on a calorimeter (one calorimetric cell = one pixel).\n",
        "- Note that although timing channel is provided, it may not necessarily help the performance of the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kr4QIMlt424u"
      },
      "outputs": [],
      "source": [
        "filename = \"./data/SingleElectronPt50_IMGCROPS_n249k_RHv1.hdf5\"\n",
        "data1 = h5py.File(filename, \"r\")\n",
        "Y1 = data1[\"y\"]\n",
        "X1 = data1[\"X\"]\n",
        "filename = \"./data/SinglePhotonPt50_IMGCROPS_n249k_RHv1.hdf5\"\n",
        "data0 = h5py.File(filename, \"r\")\n",
        "Y0 = data0[\"y\"]\n",
        "X0 = data0[\"X\"]\n",
        "X_final = np.concatenate((X0[:], X1[:]), axis=0)\n",
        "X_final = np.moveaxis(X_final, 3, 1)\n",
        "Y_final = np.concatenate((Y0[:], Y1[:]), axis=0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-JpHCOf38fDL"
      },
      "source": [
        "# Configure Training / Validation / Test Sets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-RTXS58x46Fq"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X_final, #Can change to X_final[:, 0:1, :, :] to use the Hit-Energy channel only\n",
        "    Y_final,\n",
        "    test_size=0.2,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "X_valid, X_test, y_valid, y_test = train_test_split(\n",
        "    X_test,\n",
        "    y_test,\n",
        "    test_size=0.5,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "print(f\"X_train shape: {X_train.shape} - y_train shape: {y_train.shape}\")\n",
        "print(f\"X_test shape: {X_test.shape} - y_test shape: {y_test.shape}\")\n",
        "print(f\"X_valid shape: {X_valid.shape} - y_valid shape: {y_valid.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UTFIMRnL8w41"
      },
      "source": [
        "# Plot sample of training images\n",
        "Note that although the timing channel is provided, it may not necessarily help the performance of the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e_IDs16U52-C"
      },
      "outputs": [],
      "source": [
        "plt.figure(1)\n",
        "\n",
        "plt.subplot(221)\n",
        "plt.imshow(X_train[1,0,:,:])\n",
        "plt.title(\"Channel 0: Energy\")  # Energy\n",
        "plt.grid(True)\n",
        "\n",
        "plt.subplot(222)\n",
        "plt.imshow(X_train[1,1,:,:])\n",
        "plt.title(\"Channel 1: Time\")  # Time\n",
        "plt.grid(True)\n",
        "\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Construct Dataloaders\n",
        "\n"
      ],
      "metadata": {
        "id": "NvJFlvyX9Mk3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ClassificationDataset(Dataset):\n",
        "    def __init__(self, X, y):\n",
        "        self.X = torch.from_numpy(X.copy()).float()\n",
        "        self.y = torch.from_numpy(y.copy()).long()\n",
        "    def __len__(self):\n",
        "        return len(self.X)\n",
        "    def __getitem__(self, idx):\n",
        "        return self.X[idx], self.y[idx]\n",
        "\n",
        "train_data = ClassificationDataset(X_train, y_train)\n",
        "valid_data = ClassificationDataset(X_valid, y_valid)\n",
        "test_data = ClassificationDataset(X_test, y_test)\n",
        "\n",
        "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
        "test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=False)\n",
        "valid_loader = DataLoader(valid_data, batch_size=batch_size, shuffle=False)"
      ],
      "metadata": {
        "id": "7H48lDzE9UUj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "del X_train, y_train, X_valid, y_valid, X_test, y_test"
      ],
      "metadata": {
        "id": "MDNTs5QkRFKT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ke_NQLiz83jZ"
      },
      "source": [
        "# Define CNN Model\n",
        "This is a sample model. You can experiment with the model and try various architectures and other models to achieve the highest possible performance.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OlIJuxXG7KDk"
      },
      "outputs": [],
      "source": [
        "### Define Convolutional Neural Network (CNN) Model ###\n",
        "\n",
        "model = nn.Sequential()\n",
        "model.append(nn.Conv2d(in_channels=2, out_channels=16, kernel_size=3, padding='same'))\n",
        "model.append(nn.ReLU())\n",
        "model.append(nn.Conv2d(in_channels=16, out_channels=16, kernel_size=3, padding='same'))\n",
        "model.append(nn.ReLU())\n",
        "model.append(nn. MaxPool2d(kernel_size=(2, 2)))\n",
        "model.append(nn.Conv2d(in_channels=16, out_channels=16, kernel_size=3, padding='same'))\n",
        "model.append(nn.ReLU())\n",
        "model.append(nn.Conv2d(in_channels=16, out_channels=16, kernel_size=3, padding='same'))\n",
        "model.append(nn.ReLU())\n",
        "model.append(nn. MaxPool2d(kernel_size=(2, 2)))\n",
        "model.append(nn.Flatten())\n",
        "model.append(nn.Linear(1024, 256))\n",
        "model.append(nn.ReLU())\n",
        "model.append(nn.Dropout(0.2))\n",
        "model.append(nn.Linear(256, 128))\n",
        "model.append(nn.ReLU())\n",
        "model.append(nn.Dropout(0.2))\n",
        "model.append(nn.Linear(128, 1))\n",
        "model.append(nn.Sigmoid())\n",
        "model.append(nn.Flatten(start_dim=0))\n",
        "print(model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bsQWFD0M86pA"
      },
      "source": [
        "## Train the Model\n",
        "You may further optimize the model, tune hyper-parameters, etc. accordingly to achieve the best performance possible."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def train_and_validate(train_loader, val_loader, model, optimizer, criterion, num_epochs, metric=None, scheduler=None, device='cpu'):\n",
        "    history = {\n",
        "        'epoch': [],\n",
        "        'train_loss': [],\n",
        "        'train_metric': [],\n",
        "        'val_loss': [],\n",
        "        'val_metric': [],\n",
        "        'learning_rate': []\n",
        "    }  # Initialize a dictionary to store epoch-wise results\n",
        "\n",
        "    model.to(device)  # Move the model to the specified device\n",
        "\n",
        "    with torch.no_grad():\n",
        "        proper_dtype = torch.int64\n",
        "        X,y = next(iter(train_loader))\n",
        "        X = X.to(device)\n",
        "        y = y.to(device)\n",
        "        try:\n",
        "            loss = criterion(model(X), y.to(proper_dtype))\n",
        "        except:\n",
        "            try:\n",
        "                proper_dtype = torch.float32\n",
        "                loss = criterion(model(X), y.to(proper_dtype))\n",
        "            except:\n",
        "                print(\"No valid data-type could be found\")\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()  # Set the model to training mode\n",
        "        epoch_loss = 0.0  # Initialize the epoch loss and metric values\n",
        "        epoch_metric = 0.0\n",
        "\n",
        "        # Training loop\n",
        "        for X, y in train_loader:\n",
        "            X = X.to(device)\n",
        "            y = y.to(device)\n",
        "            y = y.to(proper_dtype)\n",
        "            optimizer.zero_grad()  # Clear existing gradients\n",
        "            outputs = model(X)  # Make predictions\n",
        "            loss = criterion(outputs, y)  # Compute the loss\n",
        "            loss.backward()  # Compute gradients\n",
        "            optimizer.step()  # Update model parameters\n",
        "\n",
        "            epoch_loss += loss.item()\n",
        "\n",
        "            # THESE LINES HAVE BEEN UPDATED TO ACCOUNT FOR DEFAULT ARGUMENTS\n",
        "            if metric is not None:\n",
        "                epoch_metric += metric(outputs, y)\n",
        "            else:\n",
        "                epoch_metric += 0.0\n",
        "\n",
        "        # Average training loss and metric\n",
        "        epoch_loss /= len(train_loader)\n",
        "        epoch_metric /= len(train_loader)\n",
        "\n",
        "        # Validation loop\n",
        "        model.eval()  # Set the model to evaluation mode\n",
        "        with torch.no_grad():  # Disable gradient calculation\n",
        "            val_loss = 0.0\n",
        "            val_metric = 0.0\n",
        "            for X_val, y_val in val_loader:\n",
        "                X_val = X_val.to(device)\n",
        "                y_val = y_val.to(device)\n",
        "                y_val = y_val.to(proper_dtype)\n",
        "                outputs_val = model(X_val)  # Make predictions\n",
        "                val_loss += criterion(outputs_val, y_val).item()  # Compute loss\n",
        "                if metric is not None:\n",
        "                    val_metric += metric(outputs_val, y_val)\n",
        "                else:\n",
        "                    val_metric += 0.0\n",
        "\n",
        "            val_loss /= len(val_loader)\n",
        "            val_metric /= len(val_loader)\n",
        "\n",
        "        # Append epoch results to history\n",
        "        history['epoch'].append(epoch)\n",
        "        history['train_loss'].append(epoch_loss)\n",
        "        history['train_metric'].append(epoch_metric)\n",
        "        history['val_loss'].append(val_loss)\n",
        "        history['val_metric'].append(val_metric)\n",
        "        history['learning_rate'].append(optimizer.param_groups[0]['lr'])\n",
        "\n",
        "        print(f'Epoch [{epoch+1}/{num_epochs}], Train Loss: {epoch_loss:.4f}, '\n",
        "              f'Train Metric: {epoch_metric:.4f}, Val Loss: {val_loss:.4f}, '\n",
        "              f'Val Metric: {val_metric:.4f}')\n",
        "\n",
        "        if scheduler is not None:\n",
        "            scheduler.step(val_loss)\n",
        "\n",
        "    return history, model"
      ],
      "metadata": {
        "id": "oe7YASKr8Eef"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test_model(model, data_loader, criterion, metric=None, device='cpu'):\n",
        "    model.to(device)  # Move the model to the specified device\n",
        "\n",
        "    model.eval()  # Set the model to evaluation mode\n",
        "\n",
        "    total_loss = 0.0  # Initialize the total loss and metric values\n",
        "    total_metric = 0.0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        proper_dtype = torch.int64\n",
        "        X,y = next(iter(data_loader))\n",
        "        X = X.to(device)\n",
        "        y = y.to(device)\n",
        "        try:\n",
        "            loss = criterion(model(X), y.to(proper_dtype))\n",
        "        except:\n",
        "            try:\n",
        "                proper_dtype = torch.float32\n",
        "                loss = criterion(model(X), y.to(proper_dtype))\n",
        "            except:\n",
        "                print(\"No valid data-type could be found\")\n",
        "\n",
        "\n",
        "    with torch.no_grad():  # Disable gradient tracking\n",
        "        for batch in data_loader:\n",
        "            X, y = batch\n",
        "            X = X.to(device)\n",
        "            y = y.to(device)\n",
        "            y = y.to(proper_dtype)\n",
        "            # Pass the data to the model and make predictions\n",
        "            outputs = model(X)\n",
        "\n",
        "            # Compute the loss\n",
        "            loss = criterion(outputs, y)\n",
        "\n",
        "            # Add the loss and metric for the batch to the total values\n",
        "            total_loss += loss.item()\n",
        "\n",
        "            if metric is not None:\n",
        "                total_metric += metric(outputs, y)\n",
        "            else:\n",
        "                total_metric += 0.0\n",
        "\n",
        "    # Average loss and metric for the entire dataset\n",
        "    avg_loss = total_loss / len(data_loader)\n",
        "    avg_metric = total_metric / len(data_loader)\n",
        "\n",
        "    print(f'Test Loss: {avg_loss:.4f}, Test Metric: {avg_metric:.4f}')\n",
        "\n",
        "    return avg_loss, avg_metric"
      ],
      "metadata": {
        "id": "rpB7q8rF8JRJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def accuracy_metric(pred, target):\n",
        "    if len(pred.shape) == 1:\n",
        "        accuracy = torch.sum(torch.eq(pred > 0.5, target)).item() / len(pred)\n",
        "    else:\n",
        "        pred = pred.argmax(dim=1)\n",
        "        accuracy = torch.sum(pred == target).item() / len(pred)\n",
        "    return accuracy"
      ],
      "metadata": {
        "id": "G1i2meDF8MS8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Note: This may take several hours without GPU**"
      ],
      "metadata": {
        "id": "6C3aGSxC2rwV"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZGuyM25L7uc0"
      },
      "outputs": [],
      "source": [
        "optimizer = torch.optim.Adam(model.parameters(), lr=lr_init)\n",
        "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor=0.2, patience=2)\n",
        "criterion = nn.BCELoss()\n",
        "history, model = train_and_validate(train_loader, valid_loader, model, optimizer, criterion, num_epochs=num_epochs, metric=accuracy_metric, scheduler=scheduler, device=device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oLoN43278_-j"
      },
      "source": [
        "## Evaluate the Model  \n",
        "Along with the model accuracy, the prefered metric for evaluation is ROC (Receiver Operating Characteristic) curve and the AUC score (Area under the ROC Curve)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ekuQLYas7xh5"
      },
      "outputs": [],
      "source": [
        "# Evaluate on validation set\n",
        "score = test_model(model, valid_loader, criterion, metric=accuracy_metric, device=device)\n",
        "print('\\nValidation loss / accuracy: %0.4f / %0.4f'%(score[0], score[1]))\n",
        "model.to(device)\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "  y_pred = [model(x[0].to(device)).to('cpu').detach().numpy() for x in valid_loader]\n",
        "labels = [x[1] for x in valid_loader]\n",
        "y_pred = np.concatenate(y_pred, axis=0)\n",
        "labels = np.concatenate(labels, axis=0)\n",
        "fpr, tpr, _ = roc_curve(labels, y_pred)\n",
        "roc_auc = auc(fpr, tpr)\n",
        "print('Validation ROC AUC:', roc_auc)\n",
        "\n",
        "# Evaluate on test set\n",
        "score = test_model(model, test_loader, criterion, metric=accuracy_metric, device=device)\n",
        "print('\\nTest loss / accuracy: %0.4f / %0.4f'%(score[0], score[1]))\n",
        "model.to(device)\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "  y_pred = [model(x[0].to(device)).to('cpu').detach().numpy() for x in test_loader]\n",
        "labels = [x[1] for x in test_loader]\n",
        "y_pred = np.concatenate(y_pred, axis=0)\n",
        "labels = np.concatenate(labels, axis=0)\n",
        "fpr, tpr, _ = roc_curve(labels, y_pred)\n",
        "roc_auc = auc(fpr, tpr)\n",
        "print('Test ROC AUC:', roc_auc)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kmSRYI0R72Wn"
      },
      "outputs": [],
      "source": [
        "plt.plot([0, 1], [0, 1], 'k--')\n",
        "#plt.legend(loc=2, prop={'size': 15})\n",
        "plt.plot(fpr, tpr, label='Model 1 (ROC-AUC = {:.3f})'.format(roc_auc))\n",
        "plt.xlabel('False positive rate')\n",
        "plt.ylabel('True positive rate')\n",
        "plt.title('ROC curve')\n",
        "plt.legend(loc='best')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nna7GkM45Y5w"
      },
      "source": [
        "# Submission format:\n",
        "Submit the Google Colab Jupyter Notebook demonstrating your solution in the similar format as illustrated in this notebook. It should contain:\n",
        "*   The final model architecture, parameters and hyper-parameters yielding the best possible performance,\n",
        "*   Its Training and Validation accuracy,\n",
        "*   ROC curve and the AUC score as shown above.\n",
        "*   Also, please submit the final trained model containing the model architecture and its trained weights along with this notebook (For example: HDF5 file, .pb file, .pt file, etc.). Either in this notebook or in a separate notebook show how to load and use your model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "56_teGfk8cWL"
      },
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
