{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ViT_ML4Sci",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1m5gG93qfz80"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ML4SCI/DeepLearnHackathon/blob/main/ParticleImagesChallenge/ViT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a><h1>DeepLearn Hackathon 2024</h1>\n",
        "<h2>Problem Statement : </h2>\n",
        "\n",
        "*   One of the important aspects of searches for new physics at the Large Hadron Collider (LHC) involves the classification of various High-Energy Particles in collision events\n",
        "*   The goal of this challenge is to implement Vision Transformers on the dataset which will classify electron and photon particles as accurately as possible based on the detector images provided in the dataset.\n",
        "      *  Dataset:\n",
        "              https://cernbox.cern.ch/index.php/s/AtBT8y4MiQYFcgc (Photons)\n",
        "              https://cernbox.cern.ch/index.php/s/FbXw3V4XNyYB3oA (Electrons)\n",
        "      *  The dataset consists of 32x32 matrices (two channels - hit energy and time) for two classes of particles electrons and photons impinging on a calorimeter.\n",
        "*   The preferred metric for evaluating the model is ROC curve (Receiver Operating Characteristic curve) and the AUC (Area Under the ROC Curve) score.\n",
        "\n",
        "## Vision Tranformers:\n",
        "The field of Computer Vision has for years been dominated by Convolutional Neural Networks (CNNs) which use filters and create feature used by a multi-layer perceptron to perform the desired classification.\n",
        "But recently this field has been incredibly revolutionized by the architecture of Vision Transformers (ViT), which through the mechanism of self-attention has proven to obtain excellent results on many tasks.\n",
        "\n",
        "### How the Vision Transformer works in a nutshell\n",
        "1. Split an image into patches\n",
        "\n",
        "2. Flatten the patches\n",
        "\n",
        "3. Produce lower-dimensional linear embeddings from the flattened patches\n",
        "\n",
        "4. Add positional embeddings\n",
        "\n",
        "5. Feed the sequence as an input to a standard transformer encoder\n",
        "\n",
        "6. Pretrain the model with image labels (fully supervised on a huge dataset)\n",
        "\n",
        "7. Finetune on the downstream dataset for image classification<br>\n",
        "\n",
        "### Arcitecture\n",
        "<img src=\"https://miro.medium.com/max/3000/1*58xYTXTkcwsu2VwmYczcrg.png\" alt=\"Trulli\" >\n",
        "\n",
        "*    Reference link to understand Vision Transformers: https://arxiv.org/pdf/2010.11929.pdf\n",
        "*   The following is an example of keras implementation of ViT, feel free to implement using keras/ pytorch.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "5E_QkihfyCNv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/drive/MyDrive/"
      ],
      "metadata": {
        "id": "-TYUqEM-y-4o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd ./Particle_Images"
      ],
      "metadata": {
        "id": "6stSYrkhzAEy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cVep_a37nHnA"
      },
      "source": [
        "1.   Mount the drive and navigate to your working folder\n",
        "2.   Download the dataset (**Check ParticleImages.ipynb for example**)\n",
        "     *    https://cernbox.cern.ch/index.php/s/AtBT8y4MiQYFcgc (Photons)\n",
        "     *     https://cernbox.cern.ch/index.php/s/FbXw3V4XNyYB3oA (Electrons)\n",
        "3.   It is recommended to use GPU for training and inference if possible.\n",
        "4.   Install the following dependencies\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N_TRXhZq19x7"
      },
      "source": [
        "!pip install -U tensorflow-addons"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q_1XQnx22Dxo"
      },
      "source": [
        "# import libraries\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "import tensorflow_addons as tfa\n",
        "from tensorflow.keras.layers import Dense, Activation\n",
        "from tensorflow.keras.layers import Dropout\n",
        "from tensorflow.keras import regularizers\n",
        "from tensorflow.keras import initializers\n",
        "from tensorflow.keras import metrics"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ODBkx1m2nxp8"
      },
      "source": [
        "Load the dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oph9Ylri2G3G"
      },
      "source": [
        "import h5py\n",
        "import numpy as np\n",
        "\n",
        "filename = \"./data/SingleElectronPt50_IMGCROPS_n249k_RHv1.hdf5\"\n",
        "data1 = h5py.File(filename, \"r\")\n",
        "Y1 = data1[\"y\"]\n",
        "X1 = data1[\"X\"]\n",
        "filename = \"./data/SinglePhotonPt50_IMGCROPS_n249k_RHv1.hdf5\"\n",
        "data0 = h5py.File(filename, \"r\")\n",
        "Y0 = data0[\"y\"]\n",
        "X0 = data0[\"X\"]\n",
        "X_final = np.concatenate((X0[:], X1[:]), axis=0)\n",
        "Y_final = np.concatenate((Y0[:], Y1[:]), axis=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s82ugEPY2MkZ"
      },
      "source": [
        "X_final.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HBU0KjpGn3zq"
      },
      "source": [
        "Prepare the data\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rh1ukYc22R1q"
      },
      "source": [
        "num_classes = 1\n",
        "input_shape = (32, 32, 1) #Using the Hit-Energy channel only\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "x_train, x_test, y_train, y_test = train_test_split(\n",
        "    X_final[:, :, :, 0:1],\n",
        "    Y_final,\n",
        "    test_size=0.2,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "print(f\"x_train shape: {x_train.shape} - y_train shape: {y_train.shape}\")\n",
        "print(f\"x_test shape: {x_test.shape} - y_test shape: {y_test.shape}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r0sZ9uqooU3w"
      },
      "source": [
        "Configure the hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LpH8ITJg2Uv7"
      },
      "source": [
        "learning_rate = 0.001\n",
        "weight_decay = 0.0001\n",
        "batch_size = 128\n",
        "num_epochs = 20\n",
        "image_size = 32 # We'll resize input images to this size\n",
        "patch_size = 4  # Size of the patches to be extract from the input images\n",
        "num_patches = (image_size // patch_size) ** 2\n",
        "projection_dim = 32\n",
        "num_heads = 4\n",
        "transformer_units = [\n",
        "    projection_dim * 2,\n",
        "    projection_dim,\n",
        "]  # Size of the transformer layers\n",
        "transformer_layers = 8\n",
        "mlp_head_units = [256,128]  # Size of the dense layers of the final classifier"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oyQwJUbkoksG"
      },
      "source": [
        "Normalize the data <br>\n",
        "<sub><sup>You can additionally augment it (following the commented part)</sup></sub>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5BQcvqCP2h9D"
      },
      "source": [
        "data_augmentation = keras.Sequential(\n",
        "    [\n",
        "        layers.Normalization(),\n",
        "        #layers.Resizing(image_size, image_size),\n",
        "        #layers.RandomFlip(\"horizontal\"),\n",
        "        #layers.RandomRotation(factor=0.02),\n",
        "        #layers.RandomZoom(\n",
        "         #   height_factor = 0.2, width_factor = 0.2\n",
        "        #),\n",
        "    ],\n",
        "    name=\"data_augmentation\",\n",
        ")\n",
        "# Compute the mean and the variance of the training data for normalization.\n",
        "data_augmentation.layers[0].adapt(x_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9MdmddezpfIM"
      },
      "source": [
        "Implement multilayer perceptron (MLP)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t79kXlVQ2s7y"
      },
      "source": [
        "def mlp(x, hidden_units, dropout_rate):\n",
        "    for units in hidden_units:\n",
        "        x = layers.Dense(units, activation=tf.nn.gelu)(x)\n",
        "        x = layers.Dropout(dropout_rate)(x)\n",
        "    return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gGrI1Ce7pl4r"
      },
      "source": [
        "Implement patch creation as a layer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ngv0eDD02nsM"
      },
      "source": [
        "class Patches(layers.Layer):\n",
        "    def __init__(self, patch_size):\n",
        "        super(Patches, self).__init__()\n",
        "        self.patch_size = patch_size\n",
        "\n",
        "    def call(self, images):\n",
        "        batch_size = tf.shape(images)[0]\n",
        "        patches = tf.image.extract_patches(\n",
        "            images=images,\n",
        "            sizes=[1, self.patch_size, self.patch_size, 1],\n",
        "            strides=[1, self.patch_size, self.patch_size, 1],\n",
        "            rates=[1, 1, 1, 1],\n",
        "            padding=\"VALID\",\n",
        "        )\n",
        "        patch_dims = patches.shape[-1]\n",
        "        patches = tf.reshape(patches, [batch_size, -1, patch_dims])\n",
        "        return patches\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BcPd8c5Fptk2"
      },
      "source": [
        "Display patches for a sample image"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QB2h57h22vn7"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure(figsize=(4, 4))\n",
        "image = x_train[0,:,:,0]\n",
        "plt.imshow(image.astype(\"float32\"))\n",
        "#plt.axis(\"off\")\n",
        "\n",
        "patches = Patches(patch_size)(x_train[0:1,:,:,0:1])\n",
        "print(f\"Image size: {image_size} X {image_size}\")\n",
        "print(f\"Patch size: {patch_size} X {patch_size}\")\n",
        "print(f\"Patches per image: {patches.shape[1]}\")\n",
        "print(f\"Elements per patch: {patches.shape[-1]}\")\n",
        "\n",
        "n = int(np.sqrt(patches.shape[1]))\n",
        "plt.figure(figsize=(4, 4))\n",
        "for i, patch in enumerate(patches[0]):\n",
        "    ax = plt.subplot(n, n, i + 1)\n",
        "    patch_img = tf.reshape(patch, (patch_size, patch_size))\n",
        "    plt.imshow(patch_img.numpy().astype(\"float32\"))\n",
        "    plt.axis(\"off\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uI65ZIsap130"
      },
      "source": [
        "Implement the patch encoding layer<br>\n",
        "<sub>The PatchEncoder layer will linearly transform a patch by projecting it into a vector of size projection_dim. In addition, it adds a learnable position embedding to the projected vector.</sub>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bCxJ-9B-2x-r"
      },
      "source": [
        "class PatchEncoder(layers.Layer):\n",
        "    def __init__(self, num_patches, projection_dim):\n",
        "        super(PatchEncoder, self).__init__()\n",
        "        self.num_patches = num_patches\n",
        "        self.projection = layers.Dense(units=projection_dim)\n",
        "        self.position_embedding = layers.Embedding(\n",
        "            input_dim=num_patches, output_dim=projection_dim\n",
        "        )\n",
        "\n",
        "    def call(self, patch):\n",
        "        positions = tf.range(start=0, limit=self.num_patches, delta=1)\n",
        "        encoded = self.projection(patch) + self.position_embedding(positions)\n",
        "        return encoded"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SLp1jpT1qP38"
      },
      "source": [
        "Build the ViT model<br>\n",
        "<sub>The ViT model consists of multiple Transformer blocks, which use the layers.MultiHeadAttention layer as a self-attention mechanism applied to the sequence of patches. The Transformer blocks produce a [batch_size, num_patches, projection_dim] tensor, which is processed via an classifier head with softmax to produce the final class probabilities output."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O1hG0Q3t23rS"
      },
      "source": [
        "def create_vit_classifier():\n",
        "    inputs = layers.Input(shape=input_shape)\n",
        "    # Augment data.\n",
        "    augmented = data_augmentation(inputs)\n",
        "    # Create patches.\n",
        "    patches = Patches(patch_size)(augmented)\n",
        "    # Encode patches.\n",
        "    encoded_patches = PatchEncoder(num_patches, projection_dim)(patches)\n",
        "\n",
        "    # Create multiple layers of the Transformer block.\n",
        "    for _ in range(transformer_layers):\n",
        "        # Layer normalization 1.\n",
        "        x1 = layers.LayerNormalization(epsilon=1e-6)(encoded_patches)\n",
        "        # Create a multi-head attention layer.\n",
        "        attention_output = layers.MultiHeadAttention(\n",
        "            num_heads=num_heads, key_dim=projection_dim, dropout=0.1\n",
        "        )(x1, x1)\n",
        "        # Skip connection 1.\n",
        "        x2 = layers.Add()([attention_output, encoded_patches])\n",
        "        # Layer normalization 2.\n",
        "        x3 = layers.LayerNormalization(epsilon=1e-6)(x2)\n",
        "        # MLP.\n",
        "        x3 = mlp(x3, hidden_units=transformer_units, dropout_rate=0.1)\n",
        "        encoded_patches = layers.Add()([x3, x2])\n",
        "\n",
        "\n",
        "    # Create a [batch_size, projection_dim] tensor.\n",
        "    representation = layers.LayerNormalization(epsilon=1e-6)(encoded_patches)\n",
        "    representation = layers.Flatten()(representation)\n",
        "    representation = layers.Dropout(0.5)(representation)\n",
        "    # Add MLP.\n",
        "    features = mlp(representation, hidden_units=mlp_head_units, dropout_rate=0.5)\n",
        "    # Classify outputs.\n",
        "    logits = layers.Dense(1)(features)\n",
        "    # Create the Keras model.\n",
        "    model = keras.Model(inputs=inputs, outputs=logits)\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JQ4mL4eFq3Rq"
      },
      "source": [
        "Compile, train, and evaluate the mode <br>\n",
        "**Note: This may take several hours if not using GPU**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EZ6IQ2du28mE"
      },
      "source": [
        "def run_experiment(model):\n",
        "    optimizer = tfa.optimizers.AdamW(\n",
        "        learning_rate=learning_rate, weight_decay=weight_decay\n",
        "    )\n",
        "\n",
        "    model.compile(\n",
        "        optimizer=optimizer,#keras.optimizers.Adam(lr=0.001),\n",
        "        loss= keras.losses.BinaryCrossentropy(from_logits=True),#'binary_crossentropy',\n",
        "        #loss= keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "        metrics=[\n",
        "            'accuracy',\n",
        "            'mae'\n",
        "        ]\n",
        "    )\n",
        "\n",
        "    checkpoint_filepath = \"/tmp/checkpoint\"\n",
        "    checkpoint_callback = keras.callbacks.ModelCheckpoint(\n",
        "        checkpoint_filepath,\n",
        "        monitor=\"val_accuracy\",\n",
        "        save_best_only=True,\n",
        "        save_weights_only=True,\n",
        "    )\n",
        "\n",
        "    history = model.fit(\n",
        "        x=x_train,\n",
        "        y=y_train,\n",
        "        batch_size=batch_size,\n",
        "        epochs=num_epochs,\n",
        "        validation_split=0.1,\n",
        "        callbacks=[checkpoint_callback],\n",
        "    )\n",
        "\n",
        "    model.load_weights(checkpoint_filepath)\n",
        "    _, accuracy, top_5_accuracy = model.evaluate(x_test, y_test)\n",
        "    print(f\"Test accuracy: {round(accuracy * 100, 2)}%\")\n",
        "    #print(f\"Test top 5 accuracy: {round(top_5_accuracy * 100, 2)}%\")\n",
        "\n",
        "    return history\n",
        "\n",
        "\n",
        "vit_classifier = create_vit_classifier()\n",
        "history = run_experiment(vit_classifier)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VRdvYgXfq5vu"
      },
      "source": [
        "Calculate the ROC score"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8NddBcaJ3FLz"
      },
      "source": [
        "from sklearn.metrics import roc_curve, auc\n",
        "# Evaluate on test set\n",
        "score = vit_classifier.evaluate(x_test, y_test, verbose=1)\n",
        "print('\\nTest loss / accuracy: %0.4f / %0.4f'%(score[0], score[1]))\n",
        "y_pred = vit_classifier.predict(x_test)\n",
        "fpr, tpr, _ = roc_curve(y_test, y_pred)\n",
        "roc_auc = auc(fpr, tpr)\n",
        "print('Test ROC AUC:', roc_auc)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u1o2vmhDrEtA"
      },
      "source": [
        "ROC Curve Plot"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gU_8kYfbYmp1"
      },
      "source": [
        "plt.plot([0, 1], [0, 1], 'k--')\n",
        "#plt.legend(loc=2, prop={'size': 15})\n",
        "plt.plot(fpr, tpr, label='Model 1 (ROC-AUC = {:.3f})'.format(roc_auc))\n",
        "plt.xlabel('False positive rate')\n",
        "plt.ylabel('True positive rate')\n",
        "plt.title('ROC curve')\n",
        "plt.legend(loc='best')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oPlXL_ofbbMp"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fOBEkTaHX1MW"
      },
      "source": [
        "# Your turn, All the Best!"
      ]
    }
  ]
}