{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ML4SCI/DeepLearnHackathon/blob/main/HiggsBosonClassificationChallenge/higgs_classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9URMW3ZkrBGi"
      },
      "source": [
        "# Higgs Classification Challenge"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T4kV4R6SDR8X"
      },
      "source": [
        "**Background:** High-energy collisions at the Large Hadron Collider (LHC) <br> produce particles that interact with particle detectors. One important task is <br>\n",
        "to classify different types of collisions based on their physics content,<br> allowing physicists to find patterns in the data and to potentially unravel new <br> discoveries."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fHw_uCCcHJqn"
      },
      "source": [
        "**Problem statement:** The discovery of the Higgs boson by CMS and ATLAS <br>\n",
        "Collaborations was announced at CERN in 2012. In this challenge, we will use <br>\n",
        "machine learning to classify events containing Higgs bosons from the background <br>\n",
        "events which do not contain Higgs bosons."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qAJnE-4uANmP"
      },
      "source": [
        "**Dataset:** The dataset is hosted by the Center for Machine Learning  <br>\n",
        "and Intelligent Systems at University of California, Irvine. <br>\n",
        "The dataset can be found on the [UCI Machine learning Repository](https://archive.ics.uci.edu/ml/datasets/HIGGS)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5u1kJGUdAZ9m"
      },
      "source": [
        "**Description:** The dataset consists of a total of 11 million labeled samples <br>\n",
        "of Higgs and background events produced by Monte Carlo simulations. Each sample <br>\n",
        "consists of 28 features. The first 21 features are kinematic properties <br>\n",
        "of the events. The last seven are functions of the first 21. The data labels <br>\n",
        "are 1 for signal (an event with Higgs bosons) and 0 for background (an event <br>\n",
        "without Higgs bosons)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "myu16dmzSBmo"
      },
      "source": [
        "**Steps to load the training dataset**   \n",
        "If you are having problems with this part in Colab, you can also download the file manually and put it in your Google Drive. You can then [connect your Google Drive to Colab](https://towardsdatascience.com/different-ways-to-connect-google-drive-to-a-google-colab-notebook-pt-1-de03433d2f7a)\n",
        "\n",
        "1. Download the dataset from the UCI website."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mrM-WOaxRWsP"
      },
      "outputs": [],
      "source": [
        "!wget https://archive.ics.uci.edu/ml/machine-learning-databases/00280/HIGGS.csv.gz"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0xHJtIbPVT-n"
      },
      "source": [
        "2. Unzip the dataset folder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2J3v5kgbSrjx"
      },
      "outputs": [],
      "source": [
        "!gzip -d HIGGS.csv.gz"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MUE2QepFVwEq"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "np.random.seed(1337)  # for reproducibility\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4GDeVfB04Qe4"
      },
      "source": [
        "**Load the file using pandas library**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jxfnd8shK0vq"
      },
      "outputs": [],
      "source": [
        "data = pd.read_csv('./HIGGS.csv', header=None)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZTnbxdjkUp73"
      },
      "source": [
        "The first column is the labels (y). The other columns are all of our inputs (X).\n",
        "\n",
        "The above dataset is a pandas dataframe. We can access the data using iloc. <br>\n",
        "After that, we can turn it into a numpy array if we want or leave it as a <br>\n",
        "pandas dataframe. **Use whatever you feel most comfortable with**.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hoRPhH9dNmCy"
      },
      "outputs": [],
      "source": [
        "X = data.iloc[:,1:]\n",
        "y = data.iloc[:,0]\n",
        "#X = X.to_numpy(dtype=float) #Convert pandas dataframe to numpy array (optional)\n",
        "#y = y.to_numpy(dtype=int)   #Convert pandas dataframe to numpy array (optional)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XYHGU33UgViE"
      },
      "outputs": [],
      "source": [
        "print(X.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r6-xG2T41Qkv"
      },
      "source": [
        "To generate the following examples we used a smaller dataset containing only <br>\n",
        "10,000 events. You may want to do something similar while getting your code <br>\n",
        "set up but you should eventually use the full dataset.\n",
        "\n",
        "**For final hackathon task submissions you must use the full test set.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V-7QmHPK1Oe1"
      },
      "outputs": [],
      "source": [
        "X = X[:11000]\n",
        "y = y[:11000]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pz3F8doFfuvn"
      },
      "outputs": [],
      "source": [
        "plt.hist(X.iloc[:,0], bins=30)\n",
        "plt.title(\"lepton pT\")\n",
        "plt.xlabel(\"lepton pT\")\n",
        "plt.ylabel(\"number of events\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hQX3GUrVUe7N"
      },
      "source": [
        "Next we can split our data into 9mil training data, 1mil validation data, 1mil <br>\n",
        "test data.\n",
        "\n",
        "For the rest of this hackathon, use `X_train`, `X_val`, `X_test` as input <br>\n",
        "data and `y_train`, `y_val`, `y_test` as output data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "50-8I-fj8jC7"
      },
      "outputs": [],
      "source": [
        "X_train, X_val1, y_train, y_val1 = train_test_split(X, y, test_size=0.0909090909, random_state=42)\n",
        "X_val, X_test, y_val, y_test = train_test_split(X_val1, y_val1, test_size=0.5, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o856c_zDfJ-S"
      },
      "outputs": [],
      "source": [
        "print(X_train.shape)\n",
        "print(X_val.shape)\n",
        "print(X_test.shape)\n",
        "print(y_train.shape)\n",
        "print(y_val.shape)\n",
        "print(y_test.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ld4APrANmSJ6"
      },
      "source": [
        "## **REMINDER: Use the Higgs dataset provided above for the Hackathon**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PexwSVQ34M_l"
      },
      "source": [
        "## Hackathon Task 1:\n",
        "\n",
        "Data: `X_train`\n",
        "\n",
        "Generate histograms of the different variables in `X_train` with proper axis <br>\n",
        "labels and titles.\n",
        "\n",
        "Detailed information on what each feature column is can be found in <br> *Attribute Information* section on the [UCI Machine learning Repository](https://archive.ics.uci.edu/ml/datasets/HIGGS). <br>\n",
        "For further information, refer to the [paper](https://www.nature.com/articles/ncomms5308) by Baldi et. al\n",
        "\n",
        "**Hint:** The first item is lepton pT.\n",
        "\n",
        "The following may be helpful:\n",
        "\n",
        "`names = [\"lepton pT\", \"lepton eta\", \"lepton phi\", \"missing energy magnitude\",` <br>\n",
        "`\"missing energy phi\", \"jet 1 pt\", \"jet 1 eta\", \"jet 1 phi\", \"jet 1 b-tag\",` <br>\n",
        "`\"jet 2 pt\", \"jet 2 eta\",\"jet 2 phi\", \"jet 2 b-tag\", \"jet 3 pt\", \"jet 3 eta\",` <br>\n",
        "`\"jet 3 phi\", \"jet 3 b-tag\", \"jet 4 pt\", \"jet 4 eta\", \"jet 4 phi\", \"jet 4 b-tag\",`<br>` \"m_jj\", \"m_jjj\", \"m_lv\", \"m_jlv\", \"m_bb\", \"m_wbb\", \"m_wwbb\"]`\n",
        "\n",
        "`for index, name in enumerate(names):`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DBJaFEoGApL5"
      },
      "source": [
        "## Hackathon Task 2:\n",
        "\n",
        "Data: `X_train`, `y_train`, `X_val`, `y_val`\n",
        "\n",
        "Train a model by fitting it to the training data. Use at least one metric <br>\n",
        "such as roc_auc_score, accuracy, etc. to analyze the model's performance on the <br>\n",
        "validation data. Using that performance metric, optimize or improve your model. <br>\n",
        "It should be clear from your notebook how you perform this optimization."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G3t0NHGQa-yb"
      },
      "source": [
        "## Hackathon Task 3:\n",
        "\n",
        "Data: `X_test`, `y_test`\n",
        "\n",
        "**Note: The test data should be used only for final performance evaluation.** <br>\n",
        "**Validation data can be used to tune your model but test data should not be** <br>\n",
        "**used for model tuning.**\n",
        "\n",
        "Without having done any optimization using the testing data set, analyze the <br>\n",
        "performance of the model on the testing data. Your analysis should include <br> [roc_auc_score](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.roc_auc_score.html), a ROC curve plot, and at least one other plot of your choice<br>\n",
        "such as precision-recall curves, confusion matrix, etc.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9euTzbAUy1ty"
      },
      "source": [
        "# Deliverables:\n",
        "\n",
        "-Fill out the pre- and post- hackathon surveys.\n",
        "**Reminder: The hackathon tasks should be done using the Higgs dataset.** <br>\n",
        "A pdf of the notebook with all three hackathon tasks completed. <br>\n",
        "A copy of your colab/jupyter notebook (.ipynb and pdf) with all three hackathon tasks completed. <br>\n",
        "\n",
        "\n",
        "File name convention: Try to give your submission files descriptive names, e.g. \"Higgs_Yourname.pdf\" and  <br>\n",
        "\"Higgs_Yourname.ipynb\".\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cYXn_2LHe5kv"
      },
      "source": [
        "# Examples\n",
        "\n",
        "The examples below use a different dataset (breast cancer diagnosis dataset) than what is provided above. Please use the Higgs data set for the hackathon.\n",
        "\n",
        "Note: The following examples are meant to provide a starting point. You are encouraged to get creative. Feel free to look back to earlier assignments for inspiration and code examples."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yGXUxuiMY4Uc"
      },
      "source": [
        "## Decision Tree Example"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "metadata": {
        "id": "2YCd_u1aWj7N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = load_breast_cancer() #Use the Higgs dataset for the hackathon\n",
        "X = dataset[\"data\"]\n",
        "y = dataset[\"target\"]"
      ],
      "metadata": {
        "id": "jNkp9ozCXClU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(X.shape)\n",
        "print(y.shape)"
      ],
      "metadata": {
        "id": "tNUQzTOQZfdj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)"
      ],
      "metadata": {
        "id": "YxGxtdyZW-7F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "classifier = AdaBoostClassifier(\n",
        "    DecisionTreeClassifier(max_depth=1),\n",
        "    n_estimators=200,\n",
        "    random_state=42\n",
        ")"
      ],
      "metadata": {
        "id": "NYF4jBySWprd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "classifier.fit(X_train, y_train)"
      ],
      "metadata": {
        "id": "ahZQZ4smWxU3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predictions = classifier.predict(X_test)"
      ],
      "metadata": {
        "id": "dKOvAzrdWzBu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_hat = classifier.predict_proba(X_test)[:, 1]"
      ],
      "metadata": {
        "id": "VwzQdVKEW3EP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "confusion_matrix(y_test, predictions)"
      ],
      "metadata": {
        "id": "PTgawZpIW5-P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "accuracy_score(y_test, predictions)"
      ],
      "metadata": {
        "id": "Vj8aIHQVZWQY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import roc_curve, auc"
      ],
      "metadata": {
        "id": "1bpEbh-Xptnd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_roc_curve(y_test, y_hat):\n",
        "    # Calculate ROC curve\n",
        "    fpr, tpr, thresholds = roc_curve(y_test, y_hat)\n",
        "    roc_auc = auc(fpr, tpr)\n",
        "    # Plot the ROC curve\n",
        "    plt.figure()\n",
        "    plt.plot(fpr, tpr, label='ROC curve (area = %0.3f)' % roc_auc)\n",
        "    plt.plot([0, 1], [0, 1], 'k--', label='Random Guessing')\n",
        "    plt.xlim([0.0, 1.0])\n",
        "    plt.ylim([0.0, 1.05])\n",
        "    plt.xlabel('False Positive Rate')\n",
        "    plt.ylabel('True Positive Rate')\n",
        "    plt.title('ROC Curve for Breast Cancer Classification')\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "    print(\"AUC:\", roc_auc)"
      ],
      "metadata": {
        "id": "Dr95lA3xpnY9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_roc_curve(y_test, y_hat)"
      ],
      "metadata": {
        "id": "rW71lZ6hXiMM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Basic Neural Network Example"
      ],
      "metadata": {
        "id": "0ORFeTP0Yu7R"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k_I4kazT3_3c"
      },
      "outputs": [],
      "source": [
        "from numpy import loadtxt\n",
        "from torch import nn\n",
        "import torch\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import sklearn.preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2rcPG-UX3kkq"
      },
      "outputs": [],
      "source": [
        "model_nn = nn.Sequential()\n",
        "model_nn.append(nn.Linear(30, 64))\n",
        "model_nn.append(nn.ReLU())\n",
        "model_nn.append(nn.Linear(64, 8))\n",
        "model_nn.append(nn.ReLU())\n",
        "model_nn.append(nn.Linear(8, 1))\n",
        "model_nn.append(nn.Flatten(start_dim=0))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = load_breast_cancer() #Use the Higgs dataset for the hackathon\n",
        "X = dataset[\"data\"]\n",
        "y = dataset[\"target\"]"
      ],
      "metadata": {
        "id": "54Z-3swHePns"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(X.shape)\n",
        "print(y.shape)"
      ],
      "metadata": {
        "id": "orlS9BCgMCkO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Note: The Higgs dataset is already pre-scaled so this step is not necessary\n",
        "# in the actual hackathon\n",
        "scaler = sklearn.preprocessing.StandardScaler()\n",
        "scaler = scaler.fit(X)\n",
        "X = scaler.transform(X)"
      ],
      "metadata": {
        "id": "sI4Z-MvfLzl2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(X.shape)\n",
        "print(y.shape)"
      ],
      "metadata": {
        "id": "dj-_nGd2ePns"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test1, y_train, y_test1 = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "X_test, X_val, y_test, y_val = train_test_split(X_test1, y_test1, test_size=0.5, random_state=42)"
      ],
      "metadata": {
        "id": "0PuqNjbeaETc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class PytorchDataset(Dataset):\n",
        "    def __init__(self, X, y):\n",
        "        self.X = torch.from_numpy(X.copy()).float()\n",
        "        self.y = torch.from_numpy(y.copy()).float()\n",
        "    def __len__(self):\n",
        "        return len(self.X)\n",
        "    def __getitem__(self, idx):\n",
        "        return self.X[idx], self.y[idx]"
      ],
      "metadata": {
        "id": "yWpqs4K-GP7q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data = PytorchDataset(X_train, y_train)\n",
        "val_data = PytorchDataset(X_val, y_val)\n",
        "test_data = PytorchDataset(X_test, y_test)\n",
        "\n",
        "train_loader = DataLoader(train_data, batch_size=5, shuffle=True)\n",
        "test_loader = DataLoader(test_data, batch_size=5, shuffle=False)\n",
        "val_loader = DataLoader(val_data, batch_size=5, shuffle=False)"
      ],
      "metadata": {
        "id": "hZZXNE_PGUSI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_and_validate(train_loader, val_loader, model, optimizer, criterion, metric, num_epochs):\n",
        "    history = {\n",
        "        'epoch': [],\n",
        "        'train_loss': [],\n",
        "        'train_metric': [],\n",
        "        'val_loss': [],\n",
        "        'val_metric': []\n",
        "    }  # Initialize a dictionary to store epoch-wise results\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()  # Set the model to training mode\n",
        "        epoch_loss = 0.0  # Initialize the epoch loss and metric values\n",
        "        epoch_metric = 0.0\n",
        "\n",
        "        # Training loop\n",
        "        for X, y in train_loader:\n",
        "            optimizer.zero_grad()  # Clear existing gradients\n",
        "            outputs = model(X)  # Make predictions\n",
        "            loss = criterion(outputs, y)  # Compute the loss\n",
        "            loss.backward()  # Compute gradients\n",
        "            optimizer.step()  # Update model parameters\n",
        "\n",
        "            epoch_loss += loss.item()\n",
        "            epoch_metric += metric(outputs, y)\n",
        "\n",
        "        # Average training loss and metric\n",
        "        epoch_loss /= len(train_loader)\n",
        "        epoch_metric /= len(train_loader)\n",
        "\n",
        "        # Validation loop\n",
        "        model.eval()  # Set the model to evaluation mode\n",
        "        with torch.no_grad():  # Disable gradient calculation\n",
        "            val_loss = 0.0\n",
        "            val_metric = 0.0\n",
        "            for X_val, y_val in val_loader:\n",
        "                outputs_val = model(X_val)  # Make predictions\n",
        "                val_loss += criterion(outputs_val, y_val).item()  # Compute loss\n",
        "                val_metric += metric(outputs_val, y_val)\n",
        "\n",
        "            val_loss /= len(val_loader)\n",
        "            val_metric /= len(val_loader)\n",
        "\n",
        "        # Append epoch results to history\n",
        "        history['epoch'].append(epoch_loss)\n",
        "        history['train_loss'].append(epoch_loss)\n",
        "        history['train_metric'].append(epoch_metric)\n",
        "        history['val_loss'].append(val_loss)\n",
        "        history['val_metric'].append(val_metric)\n",
        "\n",
        "        print(f'Epoch [{epoch+1}/{num_epochs}], Train Loss: {epoch_loss:.4f}, '\n",
        "              f'Train Metric: {epoch_metric:.4f}, Val Loss: {val_loss:.4f}, '\n",
        "              f'Val Metric: {val_metric:.4f}')\n",
        "\n",
        "    return history, model"
      ],
      "metadata": {
        "id": "-JCcP-JYFipH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "criterion = torch.nn.BCEWithLogitsLoss()\n",
        "optimizer = torch.optim.Adam(model_nn.parameters(), lr=0.001)\n",
        "\n",
        "def accuracy_metric(target, pred):\n",
        "    target = target.sigmoid().round()\n",
        "    return torch.sum(pred == target).item() / len(pred)"
      ],
      "metadata": {
        "id": "Ev2ZkxlPGEe-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gDEm75CK3-TT"
      },
      "outputs": [],
      "source": [
        "history, model_nn = train_and_validate(train_loader, val_loader, model_nn,\n",
        "                                       optimizer=optimizer, criterion=criterion,\n",
        "                                       metric=accuracy_metric, num_epochs=20)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "predictions = model_nn(test_loader.dataset.X).detach().numpy()"
      ],
      "metadata": {
        "id": "45eqUaPcab35"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_roc_curve(y_test, y_hat):\n",
        "    # Calculate ROC curve\n",
        "    fpr, tpr, thresholds = roc_curve(y_test, y_hat)\n",
        "    roc_auc = auc(fpr, tpr)\n",
        "    # Plot the ROC curve\n",
        "    plt.figure()\n",
        "    plt.plot(fpr, tpr, label='ROC curve (area = %0.3f)' % roc_auc)\n",
        "    plt.plot([0, 1], [0, 1], 'k--', label='Random Guessing')\n",
        "    plt.xlim([0.0, 1.0])\n",
        "    plt.ylim([0.0, 1.05])\n",
        "    plt.xlabel('False Positive Rate')\n",
        "    plt.ylabel('True Positive Rate')\n",
        "    plt.title('ROC Curve for Breast Cancer Classification')\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "    print(\"AUC:\", roc_auc)"
      ],
      "metadata": {
        "id": "b7IgMy2gaRca"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_roc_curve(y_test, predictions)"
      ],
      "metadata": {
        "id": "IsO48lXDyq3b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Saving objects to drive"
      ],
      "metadata": {
        "id": "PoM84lwVvCPc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Connecting notebook to google drive\n",
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")"
      ],
      "metadata": {
        "id": "LQU-3_qsvU0A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Saving serializable decision tree object\n",
        "import pickle as pkl\n",
        "with open(\"/content/drive/MyDrive/classifier.pkl\", \"wb\") as f:\n",
        "    pkl.dump(classifier, f)"
      ],
      "metadata": {
        "id": "qvgIVAkCozH6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Loading serializable decision tree object\n",
        "with open(\"/content/drive/MyDrive/classifier.pkl\", \"rb\") as f:\n",
        "    new_classifier = pkl.load(f)"
      ],
      "metadata": {
        "id": "Hhkc--Hcv8rB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Saving neural network model weights using pytorch\n",
        "torch.save(model_nn.state_dict(), \"/content/drive/MyDrive/my_pytorch_model.h5\")##Saving model weights"
      ],
      "metadata": {
        "id": "QlzYaHTrvR1M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Loading neural network model weights using pytorch\n",
        "new_model_nn = nn.Sequential()\n",
        "new_model_nn.append(nn.Linear(30, 64))\n",
        "new_model_nn.append(nn.ReLU())\n",
        "new_model_nn.append(nn.Linear(64, 8))\n",
        "new_model_nn.append(nn.ReLU())\n",
        "new_model_nn.append(nn.Linear(8, 1))\n",
        "new_model_nn.append(nn.Flatten(start_dim=0))\n",
        "\n",
        "new_model_nn.load_state_dict(torch.load(\"/content/drive/MyDrive/my_pytorch_model.h5\"))"
      ],
      "metadata": {
        "id": "d4j85Ftlv_8e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Saving numpy array of predictions\n",
        "np.save(\"/content/drive/MyDrive/predictions.npy\", predictions)"
      ],
      "metadata": {
        "id": "KlDIDyLpvhl6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Loading numpy array of predictions\n",
        "saved_predictions = np.load(\"/content/drive/MyDrive/predictions.npy\")"
      ],
      "metadata": {
        "id": "DsKTqS9xwl0U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "TKOM3UbOjouO"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    },
    "latex_envs": {
      "LaTeX_envs_menu_present": true,
      "autoclose": false,
      "autocomplete": true,
      "bibliofile": "biblio.bib",
      "cite_by": "apalike",
      "current_citInitial": 1,
      "eqLabelWithNumbers": true,
      "eqNumInitial": 1,
      "hotkeys": {
        "equation": "Ctrl-E",
        "itemize": "Ctrl-I"
      },
      "labels_anchors": false,
      "latex_user_defs": false,
      "report_style_numbering": false,
      "user_envs_cfg": false
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}